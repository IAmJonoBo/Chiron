#!/usr/bin/env python3
"""
Compute code hotspots as (complexity_sum × churn) for Python files.

This script provides standalone hotspot analysis using radon for complexity
and git churn data. It complements the built-in `chiron tools refactor hotspots`
command and can be used in CI/CD pipelines.

Requirements:
- radon: pip install radon
- churn.txt: Generated by git_churn.sh

Usage:
    python hotspots_py.py [src_dir] [--churn-file FILE] [--output FILE]

Example:
    python hotspots_py.py src/chiron --output hotspots.csv
"""

import argparse
import csv
import json
import pathlib
import subprocess
import sys
from typing import Dict, List, Tuple


def parse_args():
    """Parse command-line arguments."""
    parser = argparse.ArgumentParser(
        description="Compute code hotspots from complexity and churn"
    )
    parser.add_argument(
        "src_dir",
        nargs="?",
        default="src",
        help="Source directory to analyze (default: src)",
    )
    parser.add_argument(
        "--churn-file",
        default="dev-toolkit/refactoring/output/churn.txt",
        help="Path to churn file from git_churn.sh",
    )
    parser.add_argument(
        "--output",
        default="dev-toolkit/refactoring/output/hotspots.csv",
        help="Output CSV file path",
    )
    parser.add_argument(
        "--min-complexity",
        type=int,
        default=0,
        help="Minimum complexity threshold",
    )
    parser.add_argument(
        "--min-churn",
        type=int,
        default=0,
        help="Minimum churn threshold",
    )
    return parser.parse_args()


def get_complexity(src_dir: str) -> Dict[str, int]:
    """
    Run radon to get cyclomatic complexity for all Python files.
    
    Returns dict mapping file paths to total complexity sum.
    """
    try:
        result = subprocess.run(
            ["radon", "cc", "-j", src_dir],
            check=True,
            capture_output=True,
            text=True,
        )
        data = json.loads(result.stdout or "{}")
        
        # Sum complexity per file
        complexity = {}
        for path, items in data.items():
            if items:
                complexity[path] = sum(item.get("complexity", 0) for item in items)
        
        return complexity
    
    except subprocess.CalledProcessError as e:
        print(f"Error running radon: {e}", file=sys.stderr)
        print("Make sure radon is installed: pip install radon", file=sys.stderr)
        sys.exit(1)
    except json.JSONDecodeError as e:
        print(f"Error parsing radon output: {e}", file=sys.stderr)
        sys.exit(1)


def read_churn(churn_file: str) -> Dict[str, int]:
    """
    Read git churn data from file.
    
    Format: <count> <file>
    Returns dict mapping file paths to churn counts.
    """
    churn = {}
    churn_path = pathlib.Path(churn_file)
    
    if not churn_path.exists():
        print(f"Warning: Churn file not found: {churn_file}", file=sys.stderr)
        print("Run git_churn.sh first to generate churn data", file=sys.stderr)
        return churn
    
    for line in churn_path.read_text().splitlines():
        line = line.strip()
        if not line:
            continue
        
        parts = line.split(maxsplit=1)
        if len(parts) == 2:
            count_str, file_path = parts
            try:
                churn[file_path] = int(count_str)
            except ValueError:
                continue
    
    return churn


def compute_hotspots(
    complexity: Dict[str, int],
    churn: Dict[str, int],
    min_complexity: int = 0,
    min_churn: int = 0,
) -> List[Tuple[str, int, int, int]]:
    """
    Compute hotspots from complexity and churn.
    
    Returns list of (file, complexity, churn, hotspot_score) tuples,
    sorted by hotspot score descending.
    """
    all_files = set(complexity.keys()) | set(churn.keys())
    
    hotspots = []
    for file_path in all_files:
        cplx = complexity.get(file_path, 0)
        ch = churn.get(file_path, 0)
        
        # Apply thresholds
        if cplx < min_complexity or ch < min_churn:
            continue
        
        score = cplx * ch
        hotspots.append((file_path, cplx, ch, score))
    
    # Sort by hotspot score descending
    hotspots.sort(key=lambda x: x[3], reverse=True)
    
    return hotspots


def write_csv(hotspots: List[Tuple[str, int, int, int]], output_file: str):
    """Write hotspots to CSV file."""
    output_path = pathlib.Path(output_file)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with output_path.open("w", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(["file", "complexity_sum", "churn", "hotspot_score"])
        writer.writerows(hotspots)


def main():
    """Main entry point."""
    args = parse_args()
    
    print(f"Analyzing: {args.src_dir}")
    print(f"Churn file: {args.churn_file}")
    print(f"Output: {args.output}")
    print()
    
    # Get complexity data
    print("Computing complexity with radon...")
    complexity = get_complexity(args.src_dir)
    print(f"Found {len(complexity)} files with complexity data")
    
    # Read churn data
    print("Reading churn data...")
    churn = read_churn(args.churn_file)
    print(f"Found {len(churn)} files with churn data")
    
    # Compute hotspots
    print("Computing hotspots...")
    hotspots = compute_hotspots(
        complexity,
        churn,
        min_complexity=args.min_complexity,
        min_churn=args.min_churn,
    )
    
    # Write output
    write_csv(hotspots, args.output)
    print(f"\n✅ Wrote {len(hotspots)} hotspots to {args.output}")
    
    # Display top 10
    if hotspots:
        print("\nTop 10 hotspots:")
        for i, (file_path, cplx, ch, score) in enumerate(hotspots[:10], 1):
            print(f"{i:2}. {file_path}")
            print(f"    complexity={cplx}, churn={ch}, hotspot={score}")


if __name__ == "__main__":
    main()
